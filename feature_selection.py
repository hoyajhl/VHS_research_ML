# -*- coding: utf-8 -*-
"""feature_selection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13eX_HgOKzxHLpPjw6SUvIF33nv44b7TM
"""

import pandas as pd
import numpy as np
import datetime as dt
import matplotlib.pyplot as plt
from tqdm import tqdm
import gc
import random

import re
from sklearn.metrics import *
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
import warnings
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
warnings.filterwarnings(action='ignore')

df= pd.read_csv('file_name.csv')
display(df.head())

df.columns

columns_x=['bgic', 'binary_dtx_anticoa', 'binary_dtx_antiplt', 'cere', 'coro',
       'Cortex', 'htx_coa', 'htx_dm', 'htx_hl', 'htx_htn', 'htx_plt',
       'htx_statin', 'hx_ca', 'hx_chd', 'hx_pad', 'hx_str', 'hx_tia', 'male',
       'a_aca_s', 'a_ba_s', 'a_cca_s', 'a_exica_s', 'a_inica_s', 'a_mca_s',
       'a_pca_s', 'a_va_s', 'delay', 'pre_mrs', 'hospital', 'ind_str', 'toast',
       'hx_htn_new', 'htn_bin', 'hx_dm_new', 'dm_bin', 'hx_hl_new', 'smok2',
       'hx_af_new', 'af_bin', 'tx_throm', 'age', 'new_BMI', 'wbc', 'tc', 'bun',
       'cr', 'hb', 'tg', 'hct', 'hdl', 'fbs', 'plt', 'ldl', 'pt', 'i_glu',
       'sbp', 'dbp']

train=df[columns_x]
target=df['END'] # 'END', 'nih_more_1', 'nih_more_3'

from sklearn.preprocessing import StandardScaler
standardScaler = StandardScaler()

train.head()

train[train.columns[40:57]] = standardScaler.fit_transform(train[train.columns[40:57]]) ## 연속형 변수만 scaling 적용
train.head()

from sklearn.model_selection import train_test_split

train_1, test_1, train_y_1, test_y_1 = train_test_split(train,target,test_size=0.3,random_state=42)
print(train_1.shape, test_1.shape, train_y_1.shape, test_y_1.shape) # 데이터 shape 확인

random_forest_model1 = RandomForestClassifier(n_estimators = 1000, # 1000번 추정
                                             max_depth = 5, # 트리 최대 깊이 5
                                             random_state = 40) # 시드값 고정
model1 = random_forest_model1.fit(train_1, train_y_1) # 학습 진행
predict1 = model1.predict(test_1) # 평가 데이터 예측
print("Accuracy: %.2f" % (accuracy_score(test_y_1, predict1) * 100), "%") # 정확도 % 계산

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,auc
 ##재현율과 정밀도가 비슷할수록 f1 score는 높아짐 (the harmonic mean of recall and precision)
model_metrics = {}
model_metrics['randomforest: Accuracy'] = accuracy_score(test_y_1, predict1) 
model_metrics['randomforest: Precision'] = precision_score(test_y_1, predict1)
model_metrics['randomforest: Recall'] = recall_score(test_y_1, predict1) 
model_metrics['randomforest: F1 score'] =f1_score(test_y_1, predict1) 
model_metrics['randomforest: AUC'] =roc_auc_score(test_y_1,model1.predict_proba(test_1)[:, 1])
pd.DataFrame([model_metrics])

from sklearn.metrics import confusion_matrix
cf = confusion_matrix(test_y_1, predict1)
print(cf)

model1.feature_importances_  ## 모델링 feature importance 값들 확인

list_column = []   ##컬럼 리스트
list_fi = []   ##featureimportance 리스트
for i,j in zip(train.columns,model1.feature_importances_):
    list_column.append(i)
    list_fi.append(j)

## feature importance 시각화 
plt.rcParams["figure.figsize"] = (5,25)
plt.figure(1)
plt.title('Feature Importances')
plt.barh(range(len(list_fi)), list_fi, color='b', align='center')
plt.yticks(range(len(list_column)), list_column)
plt.xlabel('Relative Importance')

## feature importance 상위 피쳐들 선택해서 df(데이터 프레임)으로 만드는 작업
df_importance = pd.DataFrame(list_column, columns=['list_column'])
df_importance
df_importance['list_fi'] = list_fi
df_importance.sort_values('list_fi',ascending=False)

##feature importance 순으로 상위 30개만 추출해서 df 반환 
df_importance.sort_values('list_fi',ascending=False)[:30]

##상위 30개 변수들 리스트
columnlist_top30=df_importance.sort_values('list_fi',ascending=False)[:30].list_column.tolist()
columnlist_top20=df_importance.sort_values('list_fi',ascending=False)[:20].list_column.tolist()
columnlist_top10=df_importance.sort_values('list_fi',ascending=False)[:10].list_column.tolist()

pip install eli5

"""## *Permutation Importance*"""

import eli5
from eli5.sklearn import PermutationImportance

perm = PermutationImportance(model1,scoring="balanced_accuracy", random_state=12).fit(test_1, test_y_1)
eli5.show_weights(perm,top=20,feature_names = train_1.columns.tolist())

perm = PermutationImportance(model1, random_state=12).fit(test_1, test_y_1)
eli5.show_weights(perm, feature_names = train_1.columns.tolist())

"""## *SHAP *"""

pip install shap

# 참고링크 : https://towardsdatascience.com/a-novel-approach-to-feature-importance-shapley-additive-explanations-d18af30fc21b
import shap
# load JS visualization code to notebook
shap.initjs()

explainer = shap.TreeExplainer(model1)
shap_values = explainer.shap_values(train_1)

shap.summary_plot(shap_values, features=train_1, feature_names=train_1.columns)

## XGB importance
참고 링크: https://mljar.com/blog/feature-importance-xgboost/

from xgboost import XGBClassifier

xgb = XGBClassifier(n_estimators=1000)
xgb.fit(train_1, train_y_1)

xgb.feature_importances_

plt.barh(train_1.columns, xgb.feature_importances_)

sorted_idx = xgb.feature_importances_.argsort()
plt.barh(train_1.columns[sorted_idx], xgb.feature_importances_[sorted_idx])
plt.xlabel("Xgboost Feature Importance")

from sklearn.inspection import permutation_importance

perm_importance = permutation_importance(xgb,test_1,test_y_1)

sorted_idx = perm_importance.importances_mean.argsort()
plt.barh(train_1.columns[sorted_idx], train_1.columns[sorted_idx])
plt.xlabel("Permutation Importance")

explainer = shap.TreeExplainer(xgb)
shap_values = explainer.shap_values(test_1)

shap.summary_plot(shap_values,test_1, plot_type="bar")

shap.summary_plot(shap_values,test_1)

xgb = XGBClassifier(n_estimators=1000)
model1=xgb.fit(train_1, train_y_1)
predict1 = model1.predict(test_1) # 평가 데이터 예측
print("Accuracy: %.2f" % (accuracy_score(test_y_1, predict1) * 100), "%") # 정확도 % 계산

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,auc
 ##재현율과 정밀도가 비슷할수록 f1 score는 높아짐 (the harmonic mean of recall and precision)
model_metrics = {}
model_metrics['randomforest: Accuracy'] = accuracy_score(test_y_1, predict1) 
model_metrics['randomforest: Precision'] = precision_score(test_y_1, predict1)
model_metrics['randomforest: Recall'] = recall_score(test_y_1, predict1) 
model_metrics['randomforest: F1 score'] =f1_score(test_y_1, predict1) 
model_metrics['randomforest: AUC'] =roc_auc_score(test_y_1,model1.predict_proba(test_1)[:, 1])
pd.DataFrame([model_metrics])

from sklearn.metrics import confusion_matrix
cf = confusion_matrix(test_y_1, predict1)
print(cf)